{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChloeAI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_zR1dAsUWg-",
        "colab_type": "text"
      },
      "source": [
        "Download chloe from github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUsykrC3UdhW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f12f763-9af7-4dab-9652-3448c7024f36"
      },
      "source": [
        "!git clone https://github.com/leocornelius/chloeAI"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'chloeAI' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpatEZt6Uy6g",
        "colab_type": "text"
      },
      "source": [
        "Copy the files into the root dirrectory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjvt2nJfZntK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2ad27d2e-1ea4-449d-bb04-c9a3fe7c095a"
      },
      "source": [
        "!cp -r chloeAI/src/. .\n",
        "!ls"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chloeAI     __init__.py\t\tmodel.py   sample_data\ttelegram_bot.py\n",
            "decoder.py  interactive_bot.py\tomegle.py  src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyPkniRUVaL_",
        "colab_type": "text"
      },
      "source": [
        "Pip install the needed libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COrXNsOpVkvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63a1ad1f-0312-4a92-83d4-4b6eb7ccc62d"
      },
      "source": [
        "!pip install -r chloeAI/requirements.txt"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.16.4\n",
            "  Using cached https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting torch==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/57/d5cceb0799c06733eefce80c395459f28970ebb9e896846ce96ab579a3f1/torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.9MB 23kB/s \n",
            "\u001b[?25hCollecting transformers==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 38.6MB/s \n",
            "\u001b[?25hCollecting python-telegram-bot==12.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/8f/e1ae8acee0398c041464ceb97be4f76819876df8585660ee402e92015d44/python_telegram_bot-12.3.0-py2.py3-none-any.whl (351kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 43.2MB/s \n",
            "\u001b[?25hCollecting python-omegle\n",
            "  Downloading https://files.pythonhosted.org/packages/0e/6f/a3dfd846ce96b365fbfb93ec3639c089873ea93b870f40add16c43661079/python_omegle-1.1.6-py3-none-any.whl\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.1MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 40.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->-r chloeAI/requirements.txt (line 3)) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->-r chloeAI/requirements.txt (line 3)) (1.14.48)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->-r chloeAI/requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->-r chloeAI/requirements.txt (line 3)) (4.41.1)\n",
            "Collecting cryptography\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/9c/647e559a6e8be493dc2a7a5d15d26cb501ca60ec299b356f23839a673a83/cryptography-3.1-cp35-abi3-manylinux2010_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from python-telegram-bot==12.3.0->-r chloeAI/requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from python-telegram-bot==12.3.0->-r chloeAI/requirements.txt (line 4)) (2020.6.20)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.6/dist-packages (from python-telegram-bot==12.3.0->-r chloeAI/requirements.txt (line 4)) (5.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->-r chloeAI/requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->-r chloeAI/requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->-r chloeAI/requirements.txt (line 3)) (0.16.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->-r chloeAI/requirements.txt (line 3)) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->-r chloeAI/requirements.txt (line 3)) (1.17.48)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->-r chloeAI/requirements.txt (line 3)) (0.3.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r chloeAI/requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r chloeAI/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r chloeAI/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->python-telegram-bot==12.3.0->-r chloeAI/requirements.txt (line 4)) (1.14.2)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->transformers==2.3.0->-r chloeAI/requirements.txt (line 3)) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->transformers==2.3.0->-r chloeAI/requirements.txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->python-telegram-bot==12.3.0->-r chloeAI/requirements.txt (line 4)) (2.20)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=3c64f42ffc23f2d4717a40426c56da070cc415d85f9828a756aa3065dda9dc27\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchvision 0.7.0+cu101 has requirement torch==1.6.0, but you'll have torch 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, torch, sacremoses, sentencepiece, transformers, cryptography, python-telegram-bot, python-omegle\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "Successfully installed cryptography-3.1 numpy-1.16.4 python-omegle-1.1.6 python-telegram-bot-12.3.0 sacremoses-0.0.43 sentencepiece-0.1.91 torch-1.2.0 transformers-2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ4uhfyVVKY3",
        "colab_type": "text"
      },
      "source": [
        "Edit the config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX3-U-YMVF17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import configparser\n",
        "\n",
        "config = configparser.ConfigParser(allow_no_value=True)\n",
        "config.read(f'chloeAI/demo_config.cfg')\n",
        "config.set('chatbot', 'telegram_token', '*YOUR_TOKEN_HERE*')\n",
        "config.set('chatbot', 'giphy_token', '*YOUR_TOKEN_HERE*')\n",
        "\n",
        "with open(f'my_chatbot.cfg', 'w') as f:\n",
        "    config.write(f)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P32QtudVWCaO",
        "colab_type": "text"
      },
      "source": [
        "Check the GPU details. If you are having issues running chloe ensure this has not changed since last time  (see https://research.google.com/colaboratory/faq.html as to why it may have done)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u6x7GraWcUT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "d074dfb0-0361-48a1-db4a-bdb94b23aab1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Sep  8 15:36:00 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVmoj10ZXErI",
        "colab_type": "text"
      },
      "source": [
        "Now we are going to download the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXF-D8K-XLAY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "ca9b8454-2a7c-412c-8f8e-3bdb8f324b1e"
      },
      "source": [
        "!python3 model.py --config my_chatbot.cfg"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-08 17:11:47.818900: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-08 17:11:49,976 - __main__ - INFO - Downloading model files to medium_multiref_ft...\n",
            "100% 293/293 [00:00<00:00, 239371.07B/s]\n",
            "100% 1042301/1042301 [00:01<00:00, 737830.32B/s]\n",
            "100% 456318/456318 [00:00<00:00, 535717.18B/s]\n",
            "100% 862954531/862954531 [02:09<00:00, 6663697.67B/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlDtlMnktocN",
        "colab_type": "text"
      },
      "source": [
        "Now we are going to run the bot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKF27gdPto0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgwI4QdUcP6X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "8de1e213-ec2c-4e38-a7dd-56e01f5a9ee9"
      },
      "source": [
        "!python interactive_bot.py --config my_chatbot.cfg"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-08 17:15:05.127836: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "/content/decoder.py:118: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
            "  assert(num_samples > 1, \"MMI requires num_samples > 1\")\n",
            "2020-09-08 17:15:07,045 - model - INFO - Downloading model files to medium_multiref_ft...\n",
            "2020-09-08 17:15:07,045 - model - INFO - Loading model from medium_multiref_ft...\n",
            "2020-09-08 17:15:21,634 - __main__ - INFO - Running the chatbot...\n",
            "Bot >>> Just start texting me. If I'm getting annoying, type \"Bye\". To quit the chat type \"Quit\".\n",
            "User >>> hey\n",
            "Bot >>> Hey there!\n",
            "User >>> how are you?\n",
            "Bot >>> Very good, how are you?\n",
            "User >>> im very well\n",
            "Bot >>> I'm glad to hear that\n",
            "User >>> what have you been up to today?\n",
            "Bot >>> Nothing much. Just woke up\n",
            "User >>> oh?\n",
            "Bot >>> I've been up since 6\n",
            "User >>> Traceback (most recent call last):\n",
            "  File \"interactive_bot.py\", line 98, in <module>\n",
            "    main()\n",
            "  File \"interactive_bot.py\", line 95, in main\n",
            "    run_chat(model, tokenizer, config, mmi_model=mmi_model, mmi_tokenizer=mmi_tokenizer)\n",
            "  File \"interactive_bot.py\", line 25, in run_chat\n",
            "    prompt = input(\"User >>> \")\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}